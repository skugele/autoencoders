{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Beta-Variational AutoEncoder ($\\beta\\text{-VAE}$)\n",
    "\n",
    "### This notebook contains a Tensorflow Keras-based Convolutional Beta-Variational Auto-Encoder (using mixture of sub-classing and functional api) trained on [Fashion MNIST](https://research.zalando.com/welcome/mission/research-projects/fashion-mnist/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    %tensorflow_version 2.x\n",
    "except:\n",
    "    pass\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gs\n",
    "import numpy as np\n",
    "import seaborn as sns; sns.set()\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify gpu\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Constants ######\n",
    "BATCH_SIZE=256\n",
    "MAX_EPOCHS=5\n",
    "PERCENT_VAL=0.2 # percentage of test data used for validation\n",
    "\n",
    "# Autoencoder Parameters\n",
    "LATENT_DIM = 128\n",
    "INPUT_SHAPE = (28, 28, 1)\n",
    "BETA = 1.2  # Penalty coefficient on KL divergence (used to pressure the latent representations to be disentangled)\n",
    "\n",
    "# Save/Load Model Constants\n",
    "SAVE_PATH = os.path.join('..', 'models', 'cvae')\n",
    "\n",
    "LOADING_WEIGHTS = False\n",
    "SAVING_WEIGHTS = False\n",
    "TRAINING = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Function Definitions ######\n",
    "def normalize(images, labels=None):\n",
    "    # Add channel\n",
    "    images = tf.expand_dims(images, -1)\n",
    "    images = tf.cast(images, tf.float32)\n",
    "    images /= 255\n",
    "    \n",
    "    return images, images\n",
    "\n",
    "def display_image(image):\n",
    "    image = image if type(image) is np.ndarray else image.numpy()\n",
    "    image = image.reshape((28,28))\n",
    "    plt.figure(figsize=(2,2))\n",
    "    plt.imshow(image, cmap=plt.cm.binary)\n",
    "    plt.colorbar()\n",
    "    plt.grid(False)\n",
    "    plt.show()\n",
    "\n",
    "def display_images(images, rows, cols, dpi=128, wspace=0, hspace=0, labels=None):\n",
    "    fig = plt.figure(dpi=dpi)\n",
    "\n",
    "    spec = gs.GridSpec(rows, cols)\n",
    "    spec.update(wspace=wspace, hspace=hspace)\n",
    "\n",
    "    i = 0\n",
    "    for image in images:\n",
    "        image = image if type(image) is np.ndarray else image.numpy()\n",
    "        image = image.reshape(image.shape[:-1]) # remove channel\n",
    "        ax = plt.subplot(spec[i])\n",
    "        \n",
    "        if labels:\n",
    "            plt.title(labels[i])\n",
    "            \n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(image, cmap=plt.cm.binary)\n",
    "        i += 1\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "    \n",
    "def split_data(data, labels, percent):\n",
    "    n = data.shape[0]\n",
    "    indices = np.random.permutation(n)\n",
    "    \n",
    "    split = int(np.ceil(n * percent))\n",
    "    \n",
    "    test_ndxs = indices[split:]\n",
    "    val_ndxs = indices[:split]\n",
    "    \n",
    "    return data[test_ndxs], labels[test_ndxs], data[val_ndxs], labels[val_ndxs]\n",
    "\n",
    "def plot_history(history, metrics, figsize=(15,10)):\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    for i, metrics in enumerate(metrics):\n",
    "        plt.subplot(1,len(metrics),i+1)\n",
    "        for metric in metrics:\n",
    "            plt.plot(range(len(history.epoch)), history.history[metric], label=metric)\n",
    "        plt.legend(loc='upper right')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd elements are labels, which we don't need\n",
    "(xtrain, ytrain), (xtest, ytest) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "xtest, ytest, xval, yval = split_data(xtest, ytest, PERCENT_VAL)\n",
    "\n",
    "n_training = xtrain.shape[0]\n",
    "n_test = xtest.shape[0]\n",
    "n_val = xval.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((xtrain, xtrain)).map(normalize).cache().shuffle(n_training//10).batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE).repeat()\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((xval, xval)).map(normalize).cache().batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((xtest, xtest)).map(normalize).cache().batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing a Few Exemplars from the **TEST** Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, _ = next(iter(test_ds))\n",
    "fig = display_images(images[0:64], rows=4, cols=16, dpi=196, labels=range(images.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the $\\beta\\text{-VAE}$ Model\n",
    "\n",
    "\n",
    "## The model contains the following components:\n",
    "\n",
    "<h3>\n",
    "<ol>\n",
    "    <li> Encoder </li>\n",
    "    <li> Latent Vector Sampler </li>\n",
    "    <li> Decoder </li>\n",
    "    <li> Loss Function (Reconstruction Loss + KL-Divergence) </li>\n",
    "</ol>\n",
    "\n",
    "    Note: Our loss function encourages a normally distributed prior $p(z)$ and a Gaussian posterior approximation $q(z|x)$.\n",
    "    \n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # layers\n",
    "        self.conv1 = tf.keras.layers.Conv2D(\n",
    "          filters=64, \n",
    "          kernel_size=2, \n",
    "          strides=(1, 1), \n",
    "          padding='same', \n",
    "          activation='relu', \n",
    "          name='encoder/conv1')\n",
    "        self.maxpool1 = tf.keras.layers.MaxPool2D(name='encoder/maxpool1')\n",
    "        self.conv2 = tf.keras.layers.Conv2D(\n",
    "          filters=32, \n",
    "          kernel_size=3, \n",
    "          strides=(1, 1), \n",
    "          padding='same', \n",
    "          activation='relu', \n",
    "          name='encoder/conv2')\n",
    "        self.maxpool2 = tf.keras.layers.MaxPool2D(name='encoder/maxpool2')\n",
    "        self.conv3 = tf.keras.layers.Conv2D(\n",
    "          filters=32, \n",
    "          kernel_size=4, \n",
    "          strides=(1, 1), \n",
    "          padding='same', \n",
    "          activation='relu', \n",
    "          name='encoder/conv3')\n",
    "        self.maxpool3 = tf.keras.layers.MaxPool2D(name='encoder/maxpool3')\n",
    "        self.flatten = tf.keras.layers.Flatten(name='encoder/flatten')        \n",
    "        self.logvar = tf.keras.layers.Dense(latent_dim, name='encoder/logvar')\n",
    "        self.mu = tf.keras.layers.Dense(latent_dim, name='encoder/mu')\n",
    "        self.sigma = tf.keras.layers.Lambda(lambda t: tf.keras.backend.exp(.5*t), name='encoder/sigma')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.maxpool3(x)\n",
    "        x = self.flatten(x)   \n",
    "        return self.mu(x), self.logvar(x), self.sigma(self.logvar(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(Sampler, self).__init__()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        mu, sigma = inputs\n",
    "        \n",
    "        batch = tf.shape(mu)[0]\n",
    "        dim = tf.shape(mu)[1]\n",
    "        \n",
    "        # Gaussian noise\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        \n",
    "        return mu + sigma * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(Decoder, self).__init__(*args, **kwargs)\n",
    "        \n",
    "        # layers\n",
    "        self.dense1 = tf.keras.layers.Dense(units=7*7*64, activation=tf.nn.relu, name='decoder/dense1')\n",
    "        self.reshape1 = tf.keras.layers.Reshape(target_shape=(7, 7, 64), name='decoder/reshape1')\n",
    "        self.trans_conv1 = tf.keras.layers.Conv2DTranspose(\n",
    "          filters=64,\n",
    "          kernel_size=2,\n",
    "          strides=(2, 2),\n",
    "          padding='same',\n",
    "          activation='relu',\n",
    "          name='decoder/deconv1')\n",
    "        self.trans_conv2 = tf.keras.layers.Conv2DTranspose(\n",
    "          filters=32,\n",
    "          kernel_size=2,\n",
    "          strides=(2, 2),\n",
    "          padding='same',\n",
    "          activation='relu',\n",
    "          name='decoder/deconv2')\n",
    "        self.trans_conv3 = tf.keras.layers.Conv2DTranspose(\n",
    "          filters=1,\n",
    "          kernel_size=2,\n",
    "          strides=(1, 1),\n",
    "          padding='same',\n",
    "          activation='sigmoid',\n",
    "          name='decoder/deconv3')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.reshape1(x)\n",
    "        x = self.trans_conv1(x)\n",
    "        x = self.trans_conv2(x)\n",
    "        return self.trans_conv3(x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE Loss Function\n",
    "<h2>\n",
    "$\n",
    "\\begin{align}\n",
    "\\DeclareMathOperator{\\ExpectedVal}{\\mathbf{E}}\n",
    "\\DeclareMathOperator{\\log}{\\operatorname{log}}\n",
    "\\mathcal{L}(\\theta, \\phi; \\mathbf{x}^{(i)}) = -D_{KL}(q_\\phi(\\mathbf{z} \\vert \\mathbf{x}^{(i)}) \\| p_\\theta(\\mathbf{z})) + \\ExpectedVal_{q_\\phi(\\mathbf{z}\\vert\\mathbf{x^{(i)}})}\\big[\\log p_\\theta(\\mathbf{x}^{(i)}\\vert \\mathbf{z})\\big],\n",
    "\\end{align}\n",
    "$ \n",
    "<br><br>\n",
    "where $D_{KL}$ is the KL-divergence.\n",
    "<br><br><br><br>\n",
    "If we assume that $p_\\theta(\\mathbf{z}) = \\mathcal{N}(0, 1) \\text{ and } q_\\phi(\\mathbf{z}\\vert \\mathbf{x}^{(i)})$ is Gaussian, then the KL-divergence can be integrated analytically and has the value\n",
    "$    \n",
    "\\begin{align}\n",
    "\\DeclareMathOperator{\\log}{\\operatorname{log}}\n",
    "D_{KL}(q_\\phi(\\mathbf{z} \\vert \\mathbf{x}^{(i)}) \\| p_\\theta(\\mathbf{z}) = -\\frac{1}{2}\\sum_{j=1}^J(1+\\log((\\sigma_j)^2) - (\\mu_j)^2 - (\\sigma_j)^2),\n",
    "\\end{align}\n",
    "$\n",
    "<br>\n",
    "where $J$ is the dimensionality of the latent vector $z$.\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(latent_dim=LATENT_DIM)\n",
    "sampler = Sampler()\n",
    "decoder = Decoder()\n",
    "\n",
    "x = tf.keras.layers.Input(shape=INPUT_SHAPE)\n",
    "mu, logvar, sigma = encoder(x)\n",
    "z = sampler((mu, sigma))\n",
    "x_recon = decoder(z) \n",
    "\n",
    "# Custom loss layer for reconstruction\n",
    "class ReconstructionLoss(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs[0]\n",
    "        x_recon = inputs[1]\n",
    "        \n",
    "        recon_loss = tf.keras.backend.sum(tf.keras.backend.binary_crossentropy(x, x_recon), axis=(1,2))\n",
    "        recon_loss = tf.keras.backend.mean(recon_loss)\n",
    "        \n",
    "        self.add_loss(recon_loss, inputs=inputs)\n",
    "        \n",
    "        return recon_loss\n",
    "\n",
    "# Custom loss layer for kl-loss\n",
    "class KLLoss(tf.keras.layers.Layer):\n",
    "    def __init__(self, beta=1.0, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.beta = beta\n",
    "\n",
    "    def call(self, inputs):\n",
    "        mu = inputs[0]\n",
    "        logvar = inputs[1]\n",
    "        \n",
    "        # KL Divergence for Gaussian Distributions (see Kingma and Welling, 2014, p.11)\n",
    "        # --> Assumes that the prior p(z) is normal and the posterior approximation q(z|x) is Gaussian\n",
    "        kl_loss = -0.5 * self.beta * tf.keras.backend.sum(\n",
    "            1.0 + logvar - tf.keras.backend.square(mu) - tf.keras.backend.exp(logvar), axis=-1)\n",
    "        kl_loss = tf.keras.backend.mean(kl_loss)\n",
    "        \n",
    "        self.add_loss(kl_loss, inputs=inputs)\n",
    "        \n",
    "        return kl_loss\n",
    "\n",
    "recon_loss = ReconstructionLoss(name='ReconLoss')([x, x_recon])\n",
    "kl_loss = KLLoss(name='KL', beta=BETA)([mu, logvar])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "model = tf.keras.Model(inputs = x, outputs = [x_recon, recon_loss, kl_loss])\n",
    "model.compile(optimizer=optimizer, loss=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Reconstruction of Sampled Images *PRIOR TO TRAINING*.\n",
    "\n",
    "### this should look like random noise (if not loading weights from a saved model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize generation before training\n",
    "if TRAINING:\n",
    "    images, _ = next(iter(test_ds))\n",
    "\n",
    "    xs, _, _ = model(images)  # reconstructed images\n",
    "\n",
    "    display_images(images=xs[0:64], rows=4, cols=16, dpi=196, labels=range(images.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train The Model and Display Metrics For Model Performance on **Training** and **Validation** Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.autograph.set_verbosity(10)\n",
    "if TRAINING:\n",
    "    early_stop_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "    history = model.fit(train_ds, \n",
    "                        callbacks=[early_stop_callback],\n",
    "                        epochs=MAX_EPOCHS, \n",
    "                        steps_per_epoch=np.ceil(n_training/BATCH_SIZE), \n",
    "                        validation_data=val_ds, \n",
    "                        validation_steps=np.ceil(n_val/BATCH_SIZE))\n",
    "    \n",
    "    plot_history(history, metrics=[('loss', 'val_loss')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVING_WEIGHTS:\n",
    "    try:\n",
    "        model.save_weights(SAVE_PATH, save_format='tf')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "elif LOADING_WEIGHTS:\n",
    "    try:\n",
    "        model.load_weights(SAVE_PATH)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Reconstruction of Sampled Images *AFTER TRAINING*.\n",
    "\n",
    "### note: these are images from the **TEST** data set: the auto-encoder was NOT trained on these!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize generation after training\n",
    "images, _ = next(iter(test_ds))\n",
    "\n",
    "xs, _, _ = model(images) # latent vectors\n",
    "\n",
    "fig = display_images(xs[0:64], rows=4, cols=16, dpi=196, labels=range(images.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison between Original Images and Reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, _ = next(iter(test_ds))\n",
    "\n",
    "mu, _, sigma = encoder(images)\n",
    "zs = sampler((mu, sigma))\n",
    "xs = decoder(zs)\n",
    "\n",
    "np.random.seed(8675309)\n",
    "idxs = np.random.uniform(0, len(images), size=10).astype(int)\n",
    "imgs = np.concatenate([images.numpy()[idxs], xs.numpy()[idxs]])\n",
    "\n",
    "fig = display_images(imgs, rows=2, cols=len(idxs), wspace=0.1, dpi=192)\n",
    "# fig.savefig('recon.svg', format='svg', dpi=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h1> Cosine Similarity Over Learned Gaussians </h1>\n",
    "<h2>\n",
    "$\n",
    "\\begin{align}\n",
    "\\text{cosine similarity}(\\vec{u}, \\vec{v}) \\equiv \\frac{\\vec{u} \\cdot \\vec{v}}{\\| u \\| \\| v \\|}\n",
    "\\end{align}\n",
    "$\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Illustration of Cosine Similarity Distance Metric "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples for cosine similarity distance metric\n",
    "\n",
    "# identical vectors\n",
    "u = np.random.rand(100)\n",
    "v = u\n",
    "\n",
    "cosine_similarity([u, v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# orthogonal vectors\n",
    "u = np.random.rand(100)\n",
    "v = np.random.rand(100)\n",
    "\n",
    "# apply Gram-Schmidt to make v orthogonal to u\n",
    "v -= v.dot(u) * u / np.linalg.norm(u)**2\n",
    "\n",
    "cosine_similarity([u, v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opposite vectors\n",
    "u = np.random.rand(100)\n",
    "v = -u\n",
    "\n",
    "cosine_similarity([u, v])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most Similar Exemplars (based on cosine similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPS = 0\n",
    "TROUSERS = 1\n",
    "PULLOVER = 2\n",
    "DRESS = 3\n",
    "COAT = 4\n",
    "SANDALS = 5\n",
    "SHIRTS = 6\n",
    "SNEAKERS = 7\n",
    "BAGS = 8\n",
    "ANKLE_BOOTS = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 10\n",
    "n_samples_per_class = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomly select test images from each class in Fashion MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(8675309)\n",
    "imgs = np.concatenate([normalize(xtest[np.random.choice(np.reshape(np.argwhere(ytest==mnist_class), -1), \n",
    "                                                        n_samples_per_class, \n",
    "                                                        replace=False)])[0] \n",
    "                       for mnist_class in range(n_classes)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = display_images(imgs, rows=10, cols=25, dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Cosine Similarity Distance Matrix Using Means ($\\vec{\\mu}$) Of Latent Prob. Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, _, sigma = encoder(imgs)\n",
    "zs = sampler((mu, sigma))\n",
    "xs = decoder(zs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_matrix = cosine_similarity(mu)\n",
    "dist_matrix = np.round(dist_matrix, decimals=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply A Sigmoidal Activation Function (denoted $\\alpha_c$) To Cosine Similarities (denoted $\\delta$)\n",
    "\n",
    "<h3>\n",
    "Ideally, we want this activation function to have the following properties:\n",
    "\n",
    "1. if $\\delta(x,y)$ is close to 1.0 then $\\delta(x,y) \\approx \\alpha_c(\\delta(x,y))$\n",
    "2. if $\\delta(x,y)$ < instantiation threshold then $\\alpha_c(\\delta(x,y)$ should be dampened towards 0.0\n",
    "                                                                         \n",
    "</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def curr_activation(x, a, b):\n",
    "    return 1.0 / (1.0 + np.exp(-a*x+b))\n",
    "\n",
    "# activation function parameters (a = 1.0, b = 0.0 gives standard logistic function)\n",
    "a = 18.0 # steepness of curve\n",
    "b = a * 0.666 # horizontal shift\n",
    "\n",
    "x = np.linspace(-1.0, 1.0, 100)\n",
    "y = curr_activation(x, a, b) \n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using_activation = True\n",
    "\n",
    "# Passing the cosine similarity through a sigmoidal activation function\n",
    "act_matrix = np.copy(dist_matrix)\n",
    "\n",
    "if using_activation:\n",
    "    act_matrix = curr_activation(act_matrix, a, b)\n",
    "    \n",
    "act_matrix = np.round(act_matrix, decimals=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Heatmap Showing $\\alpha_c$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_SIZE = 10\n",
    "MEDIUM_SIZE = 11\n",
    "BIGGER_SIZE = 12\n",
    "\n",
    "plt.rcParams['font.family'] ='Times New Roman'\n",
    "\n",
    "plt.rc('font', size=BIGGER_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', labelsize=BIGGER_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca()\n",
    "im = ax.imshow(act_matrix, cmap='hot', interpolation='gaussian')\n",
    "ax.set_xticks(np.arange(0,len(imgs)+1,int((len(imgs))/10)))\n",
    "ax.set_yticks(np.arange(0,len(imgs)+1,int((len(imgs))/10)))\n",
    "\n",
    "ax.tick_params(axis='both', which='both', length=0)\n",
    "ax.set_xticklabels([])\n",
    "ax.set_yticklabels([])\n",
    "ax.grid(which='major', color='gray', linestyle='-', linewidth=0.25)\n",
    "\n",
    "cb = plt.colorbar(im)\n",
    "cb.set_ticks([])\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "# uncomment to export figure\n",
    "# fig.savefig('curr_act_heatmap.svg', format='svg', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the Most (or Least) Similar Images for a Given Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_similar(images, index, act_matrix, n, reversed=False):\n",
    "    \n",
    "    # ranks images by distance from reference image (descending order)\n",
    "    ranked_indices_with_dist = np.array(\n",
    "        sorted(\n",
    "            np.array(list(enumerate(act_matrix[index]))), # matrix of image indices and their corresponding current activations\n",
    "            key=lambda x: x[1], # key on activation\n",
    "            reverse=not reversed)) # if reversed then least similar appear first in result\n",
    "    \n",
    "    return ranked_indices_with_dist[0:n,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "ref_ndxs = range(0,n_samples_per_class*n_classes, n_samples_per_class)\n",
    "\n",
    "curr_class = 0\n",
    "for ndx in ref_ndxs:\n",
    "    sims = get_n_similar(imgs, ndx, act_matrix, n)\n",
    "    \n",
    "    indices = sims[:,0].astype(int)\n",
    "    dists = list(map(str, np.round(sims[:,1], decimals=2)))\n",
    "    \n",
    "    fig = display_images(imgs[indices], rows=1, cols=len(indices), labels=dists, dpi=100)\n",
    "    \n",
    "    # uncomment to export figure\n",
    "    # fig.savefig('{cls}_distance_from_ref_image.svg'.format(cls=curr_class), format='svg', dpi=100)\n",
    "    \n",
    "    curr_class += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-nearest neighbors (KNN) using latent similarities and activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_exemplars = dist_matrix.shape[0]\n",
    "\n",
    "conf_matrix = np.zeros(shape=(n_classes + 1, n_classes + 1))\n",
    "k = 5\n",
    "threhold = 0.0 # instantiation threshold\n",
    "\n",
    "UNK_VALUE = np.array([[n_exemplars + 1, 0.0]])\n",
    "\n",
    "for ndx in range(n_exemplars):\n",
    "    \n",
    "    # k most similar (removing \"best match\", which will be a self-reference)\n",
    "    k_similar_set = get_n_similar(imgs, ndx, act_matrix, k + 1)[1:]\n",
    "    \n",
    "    # remove examples below threshold\n",
    "    k_similar_set = k_similar_set[k_similar_set[:,1] > threhold]\n",
    "    \n",
    "    # if all examples below threshold add UNKNOWN index\n",
    "    if len(k_similar_set) == 0:\n",
    "        k_similar_set = UNK_VALUE\n",
    "               \n",
    "    # calculate the object classes from indicies for most (cosine) similar\n",
    "    obj_classes = k_similar_set[:,0].astype(int) // n_samples_per_class\n",
    "    \n",
    "    # determine best object class as object class with max number of members in most_similar_set\n",
    "    predicted_class = np.argmax(np.bincount(obj_classes))\n",
    "    actual_class = ndx // n_samples_per_class \n",
    "                \n",
    "    conf_matrix[actual_class,predicted_class] += 1\n",
    "        \n",
    "print('overall accuracy: ', sum(np.diag(conf_matrix[0:10, 0:10]))/ np.sum(conf_matrix[0:10, 0:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_SIZE = 10\n",
    "MEDIUM_SIZE = 11\n",
    "BIGGER_SIZE = 12\n",
    "\n",
    "plt.rcParams['font.family'] ='Times New Roman'\n",
    "\n",
    "plt.rc('font', size=BIGGER_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', labelsize=BIGGER_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "ax = sns.heatmap(conf_matrix, annot=True, vmax=25, linewidths=.5, cmap=\"gist_gray\", xticklabels=True, yticklabels=True)\n",
    "ax.set_xlabel(\"Object Class (Predicted)\")\n",
    "ax.set_ylabel(\"Object Class (Actual)\")\n",
    "\n",
    "# fix issue with plot cutting off top and bottom of image\n",
    "b, t = ax.get_ylim()\n",
    "b += 0.5\n",
    "t -= 0.5\n",
    "ax.set_ylim(b,t)\n",
    "\n",
    "plt.show()\n",
    "# fig.savefig('knn_confmatrix.svg', format='svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing (Cosine) Similarity Between $\\vec{\\mu}$ Of Images and Their Reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mus, _, sigmas = encoder(imgs)\n",
    "zs = sampler((mus, sigmas))\n",
    "recons = decoder(zs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_mus, _, sigma = encoder(recons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For a Single Instance..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image_and_recon(ndx):\n",
    "    orig = imgs[ndx]\n",
    "    orig_mu = np.expand_dims(mus[ndx], 0)\n",
    "\n",
    "    recon = recons[ndx]\n",
    "    recon_mu = np.expand_dims(recon_mus[ndx], 0)\n",
    "\n",
    "    fig = display_images([orig, recon], rows=1, cols=2, dpi=90, labels=['original', 'reconstruction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image_and_recon(ndx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity Matrix Between $\\vec{\\mu}$ for Images and Their Reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_dist_matrix = cosine_similarity(recon_mus, mus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most and Least Similar (Between Images and Their Reconstructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndx_least_similar, ndx_most_similar = np.argmin(np.diag(recon_dist_matrix)), np.argmax(np.diag(recon_dist_matrix))\n",
    "\n",
    "display_image_and_recon(ndx_most_similar)\n",
    "display_image_and_recon(ndx_least_similar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Heatmap Between Reconstructions and Original Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_act_matrix = np.copy(recon_dist_matrix)\n",
    "\n",
    "# Passing the cosine similarity through a sigmoidal activation function\n",
    "recon_act_matrix = 1.0 / (1.0 + np.exp(-15.0*recon_act_matrix+10.0))\n",
    "recon_act_matrix = np.round(recon_act_matrix, decimals=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_SIZE = 10\n",
    "MEDIUM_SIZE = 11\n",
    "BIGGER_SIZE = 12\n",
    "\n",
    "plt.rcParams['font.family'] ='Times New Roman'\n",
    "\n",
    "plt.rc('font', size=BIGGER_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', labelsize=BIGGER_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca()\n",
    "im = ax.imshow(recon_act_matrix, cmap='hot', interpolation='gaussian')\n",
    "ax.set_xticks(np.arange(0,len(imgs)+1,int((len(imgs))/10)))\n",
    "ax.set_yticks(np.arange(0,len(imgs)+1,int((len(imgs))/10)))\n",
    "\n",
    "ax.tick_params(axis='both', which='both', length=0)\n",
    "ax.set_xticklabels([])\n",
    "ax.set_yticklabels([])\n",
    "ax.grid(which='major', color='gray', linestyle='-', linewidth=0.25)\n",
    "\n",
    "cb = plt.colorbar(im)\n",
    "cb.set_ticks([])\n",
    "    \n",
    "plt.show()\n",
    "fig.savefig('recon_curr_act_heatmap.svg', format='svg', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(np.diag(recon_act_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_exemplars = recon_act_matrix.shape[0]\n",
    "\n",
    "conf_matrix = np.zeros(shape=(n_classes + 1, n_classes + 1))\n",
    "k = 5\n",
    "act_threhold = 0.0\n",
    "\n",
    "UNK_VALUE = np.array([[n_exemplars + 1, 0.0]])\n",
    "\n",
    "for ndx in range(n_exemplars):\n",
    "    \n",
    "    # k most similar (removing \"best match\", which will be a self-reference)\n",
    "    k_similar_set = get_n_similar(imgs, ndx, recon_act_matrix, k + 1)[1:]\n",
    "    \n",
    "    # remove examples below threshold\n",
    "    k_similar_set = k_similar_set[k_similar_set[:,1] > act_threhold]\n",
    "    \n",
    "    # if all examples below threshold add UNKNOWN index\n",
    "    if len(k_similar_set) == 0:\n",
    "        k_similar_set = UNK_VALUE\n",
    "               \n",
    "    # calculate the object classes from indicies for most (cosine) similar\n",
    "    obj_classes = k_similar_set[:,0].astype(int) // n_samples_per_class\n",
    "    \n",
    "    # determine best object class as object class with max number of members in most_similar_set\n",
    "    predicted_class = np.argmax(np.bincount(obj_classes))\n",
    "    actual_class = ndx // n_samples_per_class \n",
    "                \n",
    "    conf_matrix[actual_class,predicted_class] += 1\n",
    "        \n",
    "print('overall accuracy: ', sum(np.diag(conf_matrix[0:10, 0:10]))/ np.sum(conf_matrix[0:10, 0:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_SIZE = 10\n",
    "MEDIUM_SIZE = 11\n",
    "BIGGER_SIZE = 12\n",
    "\n",
    "plt.rcParams['font.family'] ='Times New Roman'\n",
    "\n",
    "plt.rc('font', size=BIGGER_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', labelsize=BIGGER_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "ax = sns.heatmap(conf_matrix, annot=True, vmax=25, linewidths=.5, cmap=\"gist_gray\", xticklabels=True, yticklabels=True)\n",
    "ax.set_xlabel(\"Object Class\")\n",
    "ax.set_ylabel(\"Object Class\")\n",
    "\n",
    "# fix issue with plot cutting off top and bottom of image\n",
    "b, t = ax.get_ylim()\n",
    "b += 0.5\n",
    "t -= 0.5\n",
    "ax.set_ylim(b,t)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# uncomment to export image\n",
    "# fig.savefig('knn_confmatrix.svg', format='svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autoencoders",
   "language": "python",
   "name": "autoencoders"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
